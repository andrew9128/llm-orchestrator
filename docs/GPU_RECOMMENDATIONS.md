# üìä –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –º–æ–¥–µ–ª—è–º –¥–ª—è —Ä–∞–∑–Ω—ã—Ö GPU

## üéØ –°–µ—Ç–∞–ø 1: 12GB VRAM (RTX 5070)
**–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è**: Vikhr-3-4B
- vLLM FP8 16K: 6-9GB, 1808 TPS
- SGLang W8A8+FP8 KV 65K: ~10GB, 2286 TPS ‚úÖ
- llama.cpp Q4_K_M 32K: ~4GB, ~200 TPS
```bash
sglang --model Vikhr-3-4B --load-format int8 --kv-cache-dtype fp8_e5m2 --context-length 65536
```

## üéØ –°–µ—Ç–∞–ø 2: 16GB VRAM (RTX A4000/4080/4090)
**–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è**: Vikhr-3-8B –∏–ª–∏ Saiga-Llama3-8B
- vLLM FP8 16K: 11-14GB, 1186 TPS ‚úÖ
- SGLang W8A8 32K: ~15GB, 1357 TPS
- llama.cpp Q5_K_M 32K: ~9GB, ~250 TPS
```bash
vllm --model Vikhr-3-8B --quantization fp8 --max-model-len 16384 --gpu-memory-utilization 0.9
```

## üéØ –°–µ—Ç–∞–ø 3: 24GB VRAM (Titan RTX/A5000)
**–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è**: Saiga-Nemo-12B
- vLLM FP8 16K: 13-15GB, 869 TPS ‚úÖ
- SGLang W8A8 65K: ~15GB, 223 TPS
- llama.cpp Q5_K_M 16K: ~14GB, ~150 TPS
```bash
vllm --model Saiga-Nemo-12B --quantization fp8 --max-model-len 16384 --gpu-memory-utilization 0.9
```

## üéØ –°–µ—Ç–∞–ø 4: 32GB VRAM (RTX A6000)
**–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è**: Saiga-Nemo-12B FP16 (–±–µ–∑ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏)
- vLLM FP16 32K: ~28GB, ~950 TPS ‚úÖ
- vLLM FP16 16K: ~24GB, ~1100 TPS
```bash
vllm --model Saiga-Nemo-12B --max-model-len 32768 --gpu-memory-utilization 0.9
```

## üéØ –°–µ—Ç–∞–ø 5: 2x16GB VRAM (Tensor Parallel)
**–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è**: lmdeploy –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ throughput
- Nemo-12B lmdeploy TP2 32K: 15.8GB√ó2, 1268 TPS ‚úÖ
- Nemo-12B SGLang TP2+FP8 32K: 15.7GB√ó2, 1193 TPS
- Llama3-8B SGLang TP2+FP8 32K: 15.8GB√ó2, 1826 TPS
- Vikhr-3-4B lmdeploy TP2 32K: 14.8GB√ó2, 2553 TPS
```bash
CUDA_VISIBLE_DEVICES=0,1 lmdeploy serve api_server Saiga-Nemo-12B --tp 2
```

## üìã –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è
- **FP16**: –ø–æ–ª–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å, 2 –±–∞–π—Ç–∞/–ø–∞—Ä–∞–º–µ—Ç—Ä
- **FP8**: ~1% –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞, 1 –±–∞–π—Ç/–ø–∞—Ä–∞–º–µ—Ç—Ä
- **W8A8/INT8**: ~2-3% –ø–æ—Ç–µ—Ä–∏, 1 –±–∞–π—Ç/–ø–∞—Ä–∞–º–µ—Ç—Ä
- **Q5/Q4 (GGUF)**: 3-5% –ø–æ—Ç–µ—Ä–∏, 0.5-0.7 –±–∞–π—Ç/–ø–∞—Ä–∞–º–µ—Ç—Ä

## üöÄ –í–∞—à–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (7 GPU)

GPU 0-2,3,6 (RTX A4000, 16GB):
```bash
CUDA_VISIBLE_DEVICES=0 vllm --model ~/llm_models/Vikhr-3-8B --quantization fp8 --max-model-len 16384 --port 8000
```

GPU 1 (RTX A5000, 24GB):
```bash
CUDA_VISIBLE_DEVICES=1 vllm --model ~/llm_models/saiga_nemo_12b --quantization fp8 --max-model-len 16384 --port 8001
```

GPU 4+5 (2x RTX A4000, TP2):
```bash
CUDA_VISIBLE_DEVICES=4,5 lmdeploy serve api_server ~/llm_models/saiga_nemo_12b --tp 2 --server-port 8002
```
