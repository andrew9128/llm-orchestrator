â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ                  LLM INFRASTRUCTURE CHEATSHEET                 â”ƒ
â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›

ğŸš€ Ğ£Ğ¡Ğ¢ĞĞĞĞ’ĞšĞ
  ./scripts/quick_start.sh        # Ğ˜Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾
  ./scripts/bootstrap_llm.sh      # Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ°
  source ~/.bashrc
  ./scripts/configure_models.sh   # ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ

ğŸ’¡ Ğ—ĞĞŸĞ£Ğ¡Ğš ĞœĞĞ”Ğ•Ğ›Ğ•Ğ™
  llm-manager start vllm --model <path> --port 8000
  llm-manager start sglang --model-path <path> --port 8001
  llm-select-model                # Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¹
  llm-select-model vllm_Vikhr-3-8B

ğŸ¯ Ğ ĞĞ¡ĞŸĞ Ğ•Ğ”Ğ•Ğ›Ğ•ĞĞ˜Ğ• ĞŸĞ GPU
  CUDA_VISIBLE_DEVICES=0 llm-manager start vllm --model <m1> --port 8000
  CUDA_VISIBLE_DEVICES=1 llm-manager start sglang --model-path <m2> --port 8001
  CUDA_VISIBLE_DEVICES=0,1 vllm --model <m3> --tensor-parallel-size 2 --port 8002

ğŸ“Š ĞœĞĞĞ˜Ğ¢ĞĞ Ğ˜ĞĞ“
  llm-manager status
  nvidia-smi
  tail -f ~/llm_engines/vllm.log
  lsof -i :8000

ğŸ›‘ Ğ£ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞ˜Ğ•
  llm-manager stop vllm
  kill -9 $(cat ~/llm_engines/vllm.pid)

ğŸŒ API
  curl http://localhost:8000/v1/models
  curl -X POST http://localhost:8000/v1/completions \
    -H "Content-Type: application/json" \
    -d '{"model": "model", "prompt": "Hello", "max_tokens": 50}'

ğŸ¥ HEALTHCHECK
  cd ~/llm_bootstrap/healthchecks
  ./healthcheck.sh > /dev/null 2>&1 &  # background
  ./setup_healthcheck.sh               # systemd
  systemctl --user start llm-healthcheck

ğŸ”„ ĞĞ‘ĞĞĞ’Ğ›Ğ•ĞĞ˜Ğ•
  conda activate vllm_env && pip install --upgrade vllm
  cd ~/llm_engines/llama.cpp && git pull && make clean && make LLAMA_CUDA=1 -j$(nproc)
